{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQZaQzWEiV6EmwImqe4oEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/airballoh87/ThinkStats2/blob/master/Run_painted_tree_bexa_boutique_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "h8sZd8Mx9fH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.auth import default\n",
        "creds, _ = default()"
      ],
      "metadata": {
        "id": "aaJAsUjOYfDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##data and stats\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "##visualis\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker  # Import the ticker module\n",
        "\n",
        "##data sources\n",
        "import gspread\n",
        "from google.colab import drive\n",
        "\n",
        "##web scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "##machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "##NLP\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "##saving models\n",
        "import joblib\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "UJ-pAja4zinL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get data"
      ],
      "metadata": {
        "id": "-iyZZiO39iOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real Data"
      ],
      "metadata": {
        "id": "1iKmFScm9m2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web Scrape from Painted Tree = Cincy only"
      ],
      "metadata": {
        "id": "DDAP_aiGAO-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your login credentials\n",
        "USERNAME = '105357'\n",
        "PASSWORD = 'WinstonWillow1!'\n",
        "\n",
        "# Headers for HTTP requests\n",
        "http_headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Initialize an empty DataFrame to accumulate the data\n",
        "accumulated_data = pd.DataFrame()\n",
        "\n",
        "# Start a session to keep the cookies\n",
        "with requests.Session() as session:\n",
        "    # Prepare login data\n",
        "    login_data = {\n",
        "        'consignorId': USERNAME,\n",
        "        'password': PASSWORD\n",
        "    }\n",
        "\n",
        "    # Send a POST request to the login endpoint\n",
        "    login_url = 'https://cincinnati.consignoraccess.com/auth'  # Adjust this URL if necessary\n",
        "    login_response = session.post(login_url, headers=http_headers, data=login_data)\n",
        "\n",
        "    # Verify login by checking the response and cookies\n",
        "    print(\"Login response status code:\", login_response.status_code)\n",
        "    print(\"Cookies after login:\", session.cookies.get_dict())\n",
        "\n",
        "    # Proceed to the data URL\n",
        "    #data_url = 'https://cincinnati.consignoraccess.com/activity'\n",
        "    data_url = 'https://cincinnati.consignoraccess.com/activity?fromDate=01/01/2023&toDate=06/30/2024&showPaidItems=true'\n",
        "    response = session.get(data_url, headers=http_headers)\n",
        "\n",
        "    # Print the response status code and content for verification\n",
        "    print(\"Data URL response status code:\", response.status_code)\n",
        "    print(\"Data URL response content snippet:\", response.content[:500])  # Print first 500 characters of the response content\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    #print(\"Soup object:\", soup.prettify()[:500])  # Print first 500 characters of the parsed HTML\n",
        "\n",
        "    table = soup.find('table', {'id': 'make-responsive'})\n",
        "\n",
        "    if table:\n",
        "        headers = [th.text.strip() for th in table.find('thead').find_all('th')]\n",
        "        rows = []\n",
        "        for row in table.find('tbody').find_all('tr'):\n",
        "            columns = row.find_all('td')\n",
        "            if columns:\n",
        "                rows.append([col.text.strip() for col in columns])\n",
        "        accumulated_data = pd.DataFrame(rows, columns=headers)\n",
        "\n",
        "      # Add new columns 'Category', 'Type', and 'Theme' with blank values\n",
        "        accumulated_data['Category'] = ''\n",
        "        accumulated_data['Type'] = ''\n",
        "        accumulated_data['Theme'] = ''\n",
        "        accumulated_data['Location'] = 'Cincinnati'\n",
        "\n",
        "        print(\"Accumulated data:\", accumulated_data)\n",
        "    else:\n",
        "        print(\"Table not found in the page.\")\n",
        "\n",
        "    # Optionally save to CSV\n",
        "    # accumulated_data.to_csv(\"sales_activity_data.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "1LRams7edh7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real data - Kentucky - from google sheet\n"
      ],
      "metadata": {
        "id": "1BrRBOTu-pPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc  = gspread.authorize(creds)\n",
        "\n",
        "gsheets = gc.open_by_url('https://docs.google.com/spreadsheets/d/1gzZ4ffnM5Uc4cCHmAigMQ4IS3xxVokBc1wM5tQjPQcM/edit?gid=85248025#gid=85248025') ##Kentucky\n",
        "sheets = gsheets.worksheet('Painted Tree Category From Website - Kentucky').get_all_values()\n",
        "\n",
        "kentucky_sales_data = pd.DataFrame(sheets[1:], columns=sheets[0])\n",
        "kentucky_sales_data['Location'] = 'Kentucky'\n",
        "kentucky_sales_data"
      ],
      "metadata": {
        "id": "Sdetcpu_4S4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###merge data together"
      ],
      "metadata": {
        "id": "kzTX-Hmy4P7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate accumulated_data with historical_sales_data as union\n",
        "union_data = pd.concat([accumulated_data, kentucky_sales_data], axis=0, ignore_index=True)\n",
        "union_data"
      ],
      "metadata": {
        "id": "clBUNnKn4Jc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean Data"
      ],
      "metadata": {
        "id": "qL4pP3UCBaey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## rows and columns - general"
      ],
      "metadata": {
        "id": "SUJy5wHmHcNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import joblib\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "new_sales_data = union_data\n",
        "\n",
        "# Clean and convert price columns to numerical values\n",
        "def clean_price(price):\n",
        "    if isinstance(price, str):\n",
        "        price = price.replace('(', '').replace(')', '').replace('$', '').replace(',', '')\n",
        "        try:\n",
        "            return float(price)\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return price\n",
        "\n",
        "new_sales_data['Retail Price'] = new_sales_data['Retail Price'].apply(clean_price)\n",
        "new_sales_data['Sold Price'] = new_sales_data['Sold Price'].apply(clean_price)\n",
        "new_sales_data['Store Amount'] = new_sales_data['Store Amount'].apply(clean_price)\n",
        "new_sales_data['Consignor Amount'] = new_sales_data['Consignor Amount'].apply(clean_price)\n",
        "\n",
        "# Drop rows with missing values in the target column 'Category'\n",
        "new_sales_data = new_sales_data.dropna(subset=['Category'])\n",
        "\n",
        "#Eliminates historical rows where the was a return\n",
        "new_sales_data = new_sales_data[(new_sales_data['Sold Price'] >= 0) #&\n",
        "                                #(new_sales_data['Store Amount'] >= 0) &\n",
        "                                #(new_sales_data['Consignor Amount'] >= 0)\n",
        "                                ]\n",
        "\n",
        "\n",
        "# Clean up the 'Category' column\n",
        "new_sales_data['Category'] = new_sales_data['Category'].str.upper()\n",
        "\n",
        "# Make specified updates to the 'Category' column\n",
        "category_updates = {\n",
        "    'MISC': 'MISCELLANEOUS',\n",
        "    'SHELL': 'DECOR',\n",
        "    'CLOTHES': 'CLOTHING',\n",
        "    'HAT': 'HATS',\n",
        "    'APPAREL': 'CLOTHING',\n",
        "    'DECOR - EASTER': 'DECOR',\n",
        "    'DECOR - VALENTINES': 'DECOR',\n",
        "    'DOGS': 'PET',\n",
        "    'HAIR ACCESSORIES': 'ACCESSORIES: HAIR',\n",
        "    'HOLIDAY - EASTER': 'DECOR',\n",
        "    'HOLIDAY DECOR - EASTER': 'DECOR',\n",
        "    'HOME KITCHEN': 'HOME',\n",
        "    'KIDS': 'KIDS ACCESSORIES',\n",
        "    'KIDS APPAREL': 'KIDS CLOTHING',\n",
        "    'KIDS HAIR': 'KIDS ACCESSORIES',\n",
        "    'KIDS HAT': 'KIDS ACCESSORIES',\n",
        "    'KIDS HATS': 'KIDS ACCESSORIES',\n",
        "    'KIDS JEWELRY': 'KIDS ACCESSORIES'\n",
        "}\n",
        "\n",
        "\n",
        "type_updates = {\n",
        "\n",
        "    'BUNNY': 'BUNNY DECOR',\n",
        "    'COAT': 'JACKET',\n",
        "    'COVER UP': 'COVERUP',\n",
        "    'COWBOY HAT': 'COWBOY',\n",
        "    'COWBOY HAT DECOR': 'COWBOY',\n",
        "    'DOG COLLAR LARGE': 'DOG COLLAR',\n",
        "    'DOG COLLAR MEDIUM': 'DOG COLLAR',\n",
        "    'DOG COLLAR SMALL' : 'DOG COLLAR',\n",
        "    'EAR MUFFS': 'EARMUFFS',\n",
        "    'HAIR CLIP': 'HAIR CLIPS',\n",
        "    'KEY CHAIN': 'KEYCHAIN',\n",
        "    'ORNAMENT': 'ORNAMENTS',\n",
        "    'WINTER HAT': 'WINTER',\n",
        "    'WINE TUMBLER': 'TUMBLER',\n",
        "    'TOTE': 'TOTE BAG',\n",
        "    'SWIM': 'SWIMWEAR',\n",
        "    'RING': 'RINGS',\n",
        "    'BEADED NECKLACE': 'NECKLACE',\n",
        "    'BOTTLE OPENER':'MISCELLANEOUS' ,\n",
        "    'SPPONS': 'SPOONS',\n",
        "    'SPOON HOLDER':'SPOONS'\n",
        "\n",
        "}\n",
        "\n",
        "theme_updates = {\n",
        "    'ANIMAL PRINT': 'ANIMALS',\n",
        "    'ART DECO': 'ART',\n",
        "    'EMBELLISHED DENIM': 'EMBELLISHED',\n",
        "    'FLOWER': 'FLORAL',\n",
        "    'FLOWERS': 'FLORAL',\n",
        "    'HORSE': 'HORSES',\n",
        "    'MOTHERS DAY': 'MOM',\n",
        "    'MUSHROOM': 'MUSHROOMS',\n",
        "    'PLANTS': 'PLANT',\n",
        "    'RHINESTONE': 'RHINESTONES',\n",
        "    'SELF ESTEEM': 'SELF CARE',\n",
        "    'ST. PATRICKS':    'ST PATRICKS DAY',\n",
        "    'ST.PATRICKS DAY': 'ST PATRICKS DAY',\n",
        "    'ST. PATRICKS DAY:': 'ST PATRICKS DAY',\n",
        "    'SWIFTIE': 'TAYLOR SWIFT',\n",
        "    'TIGER':'TIGER/BENGALS',\n",
        "    'BENGALS': 'TIGER/BENGALS',\n",
        "    'VALENTINE''S DAY':'VALENTINES',\n",
        "    'VALENTINE': 'VALENTINES',\n",
        "    'WINTER COAT':'WINTER',\n",
        "    'WESTER':'WESTERN'\n",
        "}\n",
        "\n",
        "new_sales_data['Category'] = new_sales_data['Category'].replace(category_updates)\n",
        "new_sales_data['Type'] = new_sales_data['Type'].replace(type_updates)\n",
        "new_sales_data['Theme'] = new_sales_data['Theme'].replace(theme_updates)\n",
        "\n",
        "##cleanup rows that aren't items\n",
        "# Drop rows where 'SKU' is blank or null\n",
        "new_sales_data = new_sales_data[new_sales_data['SKU'].notna() & (new_sales_data['SKU'] != '')]\n",
        "\n",
        "\n",
        "new_sales_data"
      ],
      "metadata": {
        "id": "ulZxhPP4BkKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run the model\n"
      ],
      "metadata": {
        "id": "DG2Ubmz-VIFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "union_data = new_sales_data\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation\n",
        "    words = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "# Filter out rows where 'Retail Price' has a value of 'Retail Price'\n",
        "model_output = union_data[union_data['Retail Price'] != 'Retail Price']\n",
        "\n",
        "model_output = model_output.drop(columns=['Type', 'Theme', 'Category'])\n",
        "\n",
        "# Clean and convert price columns to numerical values\n",
        "def clean_price(price):\n",
        "    if isinstance(price, str):\n",
        "        price = price.replace('(', '').replace(')', '').replace('$', '').replace(',', '')\n",
        "        try:\n",
        "            return float(price)\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return price\n",
        "\n",
        "model_output['Retail Price'] = model_output['Retail Price'].apply(clean_price)\n",
        "model_output['Sold Price'] = model_output['Sold Price'].apply(clean_price)\n",
        "model_output['Store Amount'] = model_output['Store Amount'].apply(clean_price)\n",
        "model_output['Consignor Amount'] = model_output['Consignor Amount'].apply(clean_price)\n",
        "\n",
        "# Apply preprocessing to the 'Description' column\n",
        "model_output['Description'] = model_output['Description'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "######category\n",
        "# Load the saved TF-IDF vectorizer\n",
        "tfidf_vectorizer = joblib.load('/content/drive/My Drive/Bexa/tfidf_vectorizer_item_category.pkl')\n",
        "# Load the pipeline (if needed)\n",
        "pipeline = joblib.load('/content/drive/My Drive/Bexa/best_model_category.pkl')\n",
        "# Transform the 'Description' column using the loaded TF-IDF vectorizer\n",
        "X_accumulated_tfidf = tfidf_vectorizer.transform(model_output['Description'])\n",
        "# Make predictions on the new data\n",
        "model_output['Predicted_Category'] = pipeline.predict(model_output['Description'])\n",
        "# Calculate prediction probabilities\n",
        "pred_probabilities = pipeline.predict_proba(model_output['Description'])\n",
        "# Get the maximum probability for each prediction\n",
        "max_probabilities = pred_probabilities.max(axis=1)\n",
        "# Add the probabilities to the DataFrame\n",
        "model_output['Category_prediction_probability'] = max_probabilities\n",
        "\n",
        "######Type\n",
        "# Load the saved TF-IDF vectorizer\n",
        "tfidf_vectorizer = joblib.load('/content/drive/My Drive/Bexa/tfidf_vectorizer_item_type.pkl')\n",
        "# Load the pipeline (if needed)\n",
        "pipeline = joblib.load('/content/drive/My Drive/Bexa/best_model_type.pkl')\n",
        "# Transform the 'Description' column using the loaded TF-IDF vectorizer\n",
        "X_accumulated_tfidf = tfidf_vectorizer.transform(model_output['Description'])\n",
        "# Make predictions on the new data\n",
        "model_output['Predicted_Type'] = pipeline.predict(model_output['Description'])\n",
        "# Calculate prediction probabilities\n",
        "pred_probabilities = pipeline.predict_proba(model_output['Description'])\n",
        "# Get the maximum probability for each prediction\n",
        "max_probabilities = pred_probabilities.max(axis=1)\n",
        "# Add the probabilities to the DataFrame\n",
        "model_output['Type_prediction_probability'] = max_probabilities\n",
        "\n",
        "\n",
        "######Theme\n",
        "# Load the saved TF-IDF vectorizer\n",
        "tfidf_vectorizer = joblib.load('/content/drive/My Drive/Bexa/tfidf_vectorizer_item_theme.pkl')\n",
        "# Load the pipeline (if needed)\n",
        "pipeline = joblib.load('/content/drive/My Drive/Bexa/best_model_theme.pkl')\n",
        "# Transform the 'Description' column using the loaded TF-IDF vectorizer\n",
        "X_accumulated_tfidf = tfidf_vectorizer.transform(model_output['Description'])\n",
        "# Make predictions on the new data\n",
        "model_output['Predicted_Theme'] = pipeline.predict(model_output['Description'])\n",
        "# Calculate prediction probabilities\n",
        "pred_probabilities = pipeline.predict_proba(model_output['Description'])\n",
        "# Get the maximum probability for each prediction\n",
        "max_probabilities = pred_probabilities.max(axis=1)\n",
        "# Add the probabilities to the DataFrame\n",
        "model_output['Theme_prediction_probability'] = max_probabilities\n",
        "\n",
        "\n",
        "\n",
        "#####final cleanup\n",
        "\n",
        "# Remove all '#' from the 'Consignor Amount' column, replace empty strings with '0', and convert to float\n",
        "model_output['Consignor Amount'] = model_output['Consignor Amount'].replace('[\\$,#]', '', regex=True)\n",
        "model_output['Consignor Amount'] = model_output['Consignor Amount'].replace('', '0').astype(float)\n",
        "\n",
        "\n",
        "# Function to ensure four-digit year\n",
        "def ensure_four_digit_year(date_str):\n",
        "    # Match two-digit year and convert to four-digit year\n",
        "    date_str = re.sub(r'(\\d{1,2}/\\d{1,2}/)(\\d{2})$', r'\\g<1>20\\2', date_str)\n",
        "    return date_str\n",
        "\n",
        "# Apply the function to the 'Date' column\n",
        "model_output['Date'] = model_output['Date'].apply(ensure_four_digit_year)\n",
        "\n",
        "# Convert the adjusted date strings to datetime objects\n",
        "model_output['Date'] = pd.to_datetime(model_output['Date'], format='%m/%d/%Y', errors='coerce')\n",
        "\n",
        "# Display the first 20 rows of the combined data\n",
        "print(model_output.head(20))\n"
      ],
      "metadata": {
        "id": "Agp4kvn9VKaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#write to file on google sheets"
      ],
      "metadata": {
        "id": "MEZr0LM-9vvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##definition for writing to google sheets"
      ],
      "metadata": {
        "id": "JpkhIS_7JcQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_dataframe_to_gsheet(dataframe, spreadsheet_name, worksheet_name):\n",
        "    # Convert the DataFrame to a list of lists\n",
        "    dataframe = dataframe.astype(str)\n",
        "    data = [dataframe.columns.values.tolist()] + dataframe.values.tolist()\n",
        "\n",
        "    # Try to open the existing spreadsheet\n",
        "    try:\n",
        "        spreadsheet = gc.open(spreadsheet_name)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        spreadsheet = gc.create(spreadsheet_name)\n",
        "\n",
        "    # Check if the worksheet already exists and delete it if it does\n",
        "    try:\n",
        "        worksheet = spreadsheet.worksheet(worksheet_name)\n",
        "        spreadsheet.del_worksheet(worksheet)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        pass\n",
        "\n",
        "    # Add a new worksheet\n",
        "    worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=len(dataframe)+1, cols=len(dataframe.columns))\n",
        "\n",
        "    # Write data to the worksheet\n",
        "    worksheet.update('A1', data)\n",
        "\n",
        "    print(\"Data written to Google Sheet successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LCgvwdaIHF_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "spreadsheet_name = 'painted_tree_ml_training'\n",
        "worksheet_name = 'Painted Tree - New model output June final - really'\n",
        "write_dataframe_to_gsheet(model_output, spreadsheet_name, worksheet_name)\n"
      ],
      "metadata": {
        "id": "WkLizNZowVqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Output for the training set"
      ],
      "metadata": {
        "id": "XGub8BYsFkaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame with the specified columns and new blank columns\n",
        "new_columns = [\n",
        "    'Date', 'Description', 'Retail Price', 'Location', 'Predicted_Category',\n",
        "    'Corrected_Category', 'Category_prediction_probability', 'Predicted_Type',\n",
        "    'Corrected_Type', 'Type_prediction_probability', 'Predicted_Theme',\n",
        "    'Corrected_Theme', 'Theme_prediction_probability'\n",
        "]\n",
        "\n",
        "# Initialize the new columns with blank values\n",
        "model_output['Corrected_Category'] = ''\n",
        "model_output['Corrected_Type'] = ''\n",
        "model_output['Corrected_Theme'] = ''\n",
        "\n",
        "# Select and reorder the columns\n",
        "selected_data = model_output[new_columns]\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the previous month and year\n",
        "current_date = datetime.now()\n",
        "first_day_of_current_month = datetime(current_date.year, current_date.month, 1)\n",
        "previous_month_end = first_day_of_current_month - pd.DateOffset(days=1)\n",
        "previous_month_start = previous_month_end.replace(day=1)\n",
        "previous_month_start, previous_month_end,selected_data['Date']\n",
        "# Filter by the previous month\n",
        "filtered_data = selected_data[\n",
        "    (selected_data['Date'] >= previous_month_start) &\n",
        "    (selected_data['Date'] <= previous_month_end)\n",
        "]\n",
        "# Filter by probability fields\n",
        "probability_filtered_data = filtered_data[\n",
        "    (filtered_data['Category_prediction_probability'] < 0.67) |\n",
        "    (filtered_data['Type_prediction_probability'] < 0.67) |\n",
        "    (filtered_data['Theme_prediction_probability'] < 0.67)\n",
        "]\n",
        "\n",
        "## print to google sheets\n",
        "spreadsheet_name = 'painted_tree_ml_training'\n",
        "worksheet_name = 'Painted Tree - Predictions Review - June'\n",
        "write_dataframe_to_gsheet(probability_filtered_data, spreadsheet_name, worksheet_name)"
      ],
      "metadata": {
        "id": "HcB5i4VYFpEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a new DataFrame with the specified columns and updated names\n",
        "updated_columns = {\n",
        "    'Date': 'Date',\n",
        "    'Description': 'Description',\n",
        "    'SKU': 'SKU',\n",
        "    'Invoice': 'Invoice',\n",
        "    'Retail Price': 'Retail Price',\n",
        "    'Sold Price': 'Sold Price',\n",
        "    'Store Amount': 'Store Amount',\n",
        "    'Consignor Amount': 'Consignor Amount',\n",
        "    'Location': 'Location',\n",
        "    'Predicted_Category': 'Category',\n",
        "    'Predicted_Type': 'Type',\n",
        "    'Predicted_Theme': 'Theme'\n",
        "}\n",
        "\n",
        "# Select and rename the columns\n",
        "updated_data = model_output.rename(columns=updated_columns)[list(updated_columns.values())]\n",
        "\n",
        "# Display the updated DataFrame\n",
        "#print(updated_data.head())\n",
        "# Usage\n",
        "spreadsheet_name = 'painted_tree_ml_training'\n",
        "worksheet_name = 'Painted Tree - New model output June FINAL for Visualization'\n",
        "write_dataframe_to_gsheet(updated_data, spreadsheet_name, worksheet_name)\n"
      ],
      "metadata": {
        "id": "cPdWWTTnI28I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data viz"
      ],
      "metadata": {
        "id": "JmH3QdE7e5US"
      }
    }
  ]
}